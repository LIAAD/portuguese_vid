{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/projects/Language-Identifier/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import datasets\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from ptvid.constants import DOMAINS, DATASET_NAME\n",
    "\n",
    "N_PROC = mp.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(example):\n",
    "    n_tokens = len(word_tokenize(example[\"text\"], language=\"portuguese\"))\n",
    "    example[\"n_tokens\"] = n_tokens\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=28): 100%|██████████| 7304438/7304438 [02:45<00:00, 44258.29 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter (num_proc=28): 100%|██████████| 7304438/7304438 [00:00<00:00, 9443693.93 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tDocs: 6859951\n",
      "\t\t\tTkns: 418642791\n",
      "\t\t\tmTkns: 6\n",
      "\t\t\tMTkns: 2042\n",
      "\t\t\tATkn: 61.03\n",
      "\t\tStdTkn: 74.03\n",
      "\t\t 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter (num_proc=28): 100%|██████████| 7304438/7304438 [00:00<00:00, 9860270.41 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tDocs: 444487\n",
      "\t\t\tTkns: 56125135\n",
      "\t\t\tmTkns: 6\n",
      "\t\t\tMTkns: 2075\n",
      "\t\t\tATkn: 126.27\n",
      "\t\tStdTkn: 205.59\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.concatenate_datasets([datasets.load_dataset(DATASET_NAME, domain, split=split) for domain in DOMAINS for split in [\"train\", \"valid\", \"test\"]])\n",
    "dataset = dataset.map(count_tokens, num_proc=N_PROC)\n",
    "    \n",
    "for label in [0, 1]:\n",
    "    print(\"\\t\\t\", label)\n",
    "    lsdata = dataset.filter(lambda x: x[\"label\"] == label, num_proc=N_PROC)\n",
    "    n_docs = len(lsdata)\n",
    "    print(f\"\\t\\t\\tDocs: {n_docs}\")\n",
    "    n_tokens = sum(lsdata['n_tokens'])\n",
    "    print(f\"\\t\\t\\tTkns: {n_tokens}\")\n",
    "    print(f\"\\t\\t\\tmTkns: {min(lsdata['n_tokens'])}\")\n",
    "    print(f\"\\t\\t\\tMTkns: {max(lsdata['n_tokens'])}\")\n",
    "    print(f\"\\t\\t\\tATkn: {n_tokens/n_docs:.02f}\")\n",
    "    print(f\"\\t\\tStdTkn: {np.std(lsdata['n_tokens']):.02f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "journalistic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t 0\n",
      "\t\t\tDocs: 1443422\n",
      "\t\t\tTkns: 189506320\n",
      "\t\t\tmTkns: 16\n",
      "\t\t\tMTkns: 475\n",
      "\t\t\tATkn: 131.29\n",
      "\t\tStdTkn: 61.45\n",
      "\t\t 1\n",
      "\t\t\tDocs: 333903\n",
      "\t\t\tTkns: 27077538\n",
      "\t\t\tmTkns: 18\n",
      "\t\t\tMTkns: 560\n",
      "\t\t\tATkn: 81.09\n",
      "\t\tStdTkn: 39.11\n",
      "literature\n",
      "\t\t 0\n",
      "\t\t\tDocs: 24090\n",
      "\t\t\tTkns: 1859660\n",
      "\t\t\tmTkns: 16\n",
      "\t\t\tMTkns: 186\n",
      "\t\t\tATkn: 77.20\n",
      "\t\tStdTkn: 37.39\n",
      "\t\t 1\n",
      "\t\t\tDocs: 52458\n",
      "\t\t\tTkns: 3805896\n",
      "\t\t\tmTkns: 17\n",
      "\t\t\tMTkns: 185\n",
      "\t\t\tATkn: 72.55\n",
      "\t\tStdTkn: 36.19\n",
      "legal\n",
      "\t\t 0\n",
      "\t\t\tDocs: 2957980\n",
      "\t\t\tTkns: 152717737\n",
      "\t\t\tmTkns: 16\n",
      "\t\t\tMTkns: 139\n",
      "\t\t\tATkn: 51.63\n",
      "\t\tStdTkn: 24.43\n",
      "\t\t 1\n",
      "\t\t\tDocs: 4653\n",
      "\t\t\tTkns: 221167\n",
      "\t\t\tmTkns: 20\n",
      "\t\t\tMTkns: 124\n",
      "\t\t\tATkn: 47.53\n",
      "\t\tStdTkn: 22.11\n",
      "politics\n",
      "\t\t 0\n",
      "\t\t\tDocs: 27887\n",
      "\t\t\tTkns: 7203739\n",
      "\t\t\tmTkns: 20\n",
      "\t\t\tMTkns: 798\n",
      "\t\t\tATkn: 258.32\n",
      "\t\tStdTkn: 173.39\n",
      "\t\t 1\n",
      "\t\t\tDocs: 3656\n",
      "\t\t\tTkns: 1012586\n",
      "\t\t\tmTkns: 21\n",
      "\t\t\tMTkns: 796\n",
      "\t\t\tATkn: 276.97\n",
      "\t\tStdTkn: 177.60\n",
      "web\n",
      "\t\t 0\n",
      "\t\t\tDocs: 43630\n",
      "\t\t\tTkns: 22598587\n",
      "\t\t\tmTkns: 22\n",
      "\t\t\tMTkns: 2042\n",
      "\t\t\tATkn: 517.96\n",
      "\t\tStdTkn: 414.72\n",
      "\t\t 1\n",
      "\t\t\tDocs: 44313\n",
      "\t\t\tTkns: 23913771\n",
      "\t\t\tmTkns: 15\n",
      "\t\t\tMTkns: 2075\n",
      "\t\t\tATkn: 539.66\n",
      "\t\tStdTkn: 463.16\n",
      "social_media\n",
      "\t\t 0\n",
      "\t\t\tDocs: 2362942\n",
      "\t\t\tTkns: 44756748\n",
      "\t\t\tmTkns: 6\n",
      "\t\t\tMTkns: 646\n",
      "\t\t\tATkn: 18.94\n",
      "\t\tStdTkn: 9.85\n",
      "\t\t 1\n",
      "\t\t\tDocs: 5504\n",
      "\t\t\tTkns: 94177\n",
      "\t\t\tmTkns: 6\n",
      "\t\t\tMTkns: 51\n",
      "\t\t\tATkn: 17.11\n",
      "\t\tStdTkn: 10.17\n"
     ]
    }
   ],
   "source": [
    "for domain in DOMAINS:\n",
    "    print(domain)\n",
    "    dataset = datasets.load_dataset(DATASET_NAME, domain)\n",
    "    dataset = dataset.map(count_tokens, num_proc=N_PROC)\n",
    "    sdata = datasets.concatenate_datasets([dataset[split] for split in [\"train\", \"valid\", \"test\"]])\n",
    "\n",
    "    for label in [0, 1]:\n",
    "        print(\"\\t\\t\", label)\n",
    "        lsdata = sdata.filter(lambda x: x[\"label\"] == label, num_proc=N_PROC)\n",
    "        n_docs = len(lsdata)\n",
    "        print(f\"\\t\\t\\tDocs: {n_docs}\")\n",
    "        n_tokens = sum(lsdata['n_tokens'])\n",
    "        print(f\"\\t\\t\\tTkns: {n_tokens}\")\n",
    "        print(f\"\\t\\t\\tmTkns: {min(lsdata['n_tokens'])}\")\n",
    "        print(f\"\\t\\t\\tMTkns: {max(lsdata['n_tokens'])}\")\n",
    "        print(f\"\\t\\t\\tATkn: {n_tokens/n_docs:.02f}\")\n",
    "        print(f\"\\t\\tStdTkn: {np.std(lsdata['n_tokens']):.02f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "journalistic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tDocs: 1777325\n",
      "\t\tTkns: 216583858\n",
      "\t\tmTkns: 16\n",
      "\t\tMTkns: 560\n",
      "\t\tATkn: 121.86\n",
      "\t\tStdTkn: 61.15\n",
      "literature\n",
      "\t\tDocs: 76548\n",
      "\t\tTkns: 5665556\n",
      "\t\tmTkns: 16\n",
      "\t\tMTkns: 186\n",
      "\t\tATkn: 74.01\n",
      "\t\tStdTkn: 36.63\n",
      "legal\n",
      "\t\tDocs: 2962633\n",
      "\t\tTkns: 152938904\n",
      "\t\tmTkns: 16\n",
      "\t\tMTkns: 139\n",
      "\t\tATkn: 51.62\n",
      "\t\tStdTkn: 24.42\n",
      "politics\n",
      "\t\tDocs: 31543\n",
      "\t\tTkns: 8216325\n",
      "\t\tmTkns: 20\n",
      "\t\tMTkns: 798\n",
      "\t\tATkn: 260.48\n",
      "\t\tStdTkn: 173.98\n",
      "web\n",
      "\t\tDocs: 87943\n",
      "\t\tTkns: 46512358\n",
      "\t\tmTkns: 15\n",
      "\t\tMTkns: 2075\n",
      "\t\tATkn: 528.89\n",
      "\t\tStdTkn: 439.93\n",
      "social_media\n",
      "\t\tDocs: 2368446\n",
      "\t\tTkns: 44850925\n",
      "\t\tmTkns: 6\n",
      "\t\tMTkns: 646\n",
      "\t\tATkn: 18.94\n",
      "\t\tStdTkn: 9.86\n"
     ]
    }
   ],
   "source": [
    "for domain in DOMAINS:\n",
    "    print(domain)\n",
    "    dataset = datasets.load_dataset(DATASET_NAME, domain)\n",
    "    dataset = dataset.map(count_tokens, num_proc=N_PROC)\n",
    "    \n",
    "    data = datasets.concatenate_datasets([dataset[split] for split in [\"train\", \"valid\", \"test\"]])\n",
    "    dataset = dataset.map(count_tokens, num_proc=N_PROC)\n",
    "\n",
    "    n_docs = len(data)\n",
    "    n_tokens = sum(data[\"n_tokens\"])\n",
    "    min_tokens = min(data[\"n_tokens\"])\n",
    "    max_tokens = max(data[\"n_tokens\"])\n",
    "    avg_tokens = n_tokens / n_docs \n",
    "\n",
    "    print(f\"\\t\\tDocs: {n_docs}\")\n",
    "    print(f\"\\t\\tTkns: {n_tokens}\")\n",
    "    print(f\"\\t\\tmTkns: {min_tokens}\")\n",
    "    print(f\"\\t\\tMTkns: {max_tokens}\")\n",
    "    print(f\"\\t\\tATkn: {avg_tokens:.02f}\")\n",
    "    print(f\"\\t\\tStdTkn: {np.std(data['n_tokens']):.02f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "journalistic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "literature\n",
      "legal\n",
      "politics\n",
      "web\n",
      "social_media\n"
     ]
    }
   ],
   "source": [
    "for domain in DOMAINS:\n",
    "    print(domain)\n",
    "    dataset = datasets.load_dataset(DATASET_NAME, domain)\n",
    "    dataset = dataset.map(count_tokens, num_proc=N_PROC)\n",
    "    \n",
    "    data = datasets.concatenate_datasets([dataset[split] for split in [\"train\", \"valid\", \"test\"]])\n",
    "    dataset = dataset.map(count_tokens, num_proc=N_PROC)\n",
    "\n",
    "    n_docs = len(data)\n",
    "    n_tokens = sum(data[\"n_tokens\"])\n",
    "    min_tokens = min(data[\"n_tokens\"])\n",
    "    max_tokens = max(data[\"n_tokens\"])\n",
    "    avg_tokens = n_tokens / n_docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
